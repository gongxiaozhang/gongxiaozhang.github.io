---
layout: post
title:  "Why neural processor ?"
categories: jekyll update
typora-copy-images-to: ../assets/img
typora-root-url: ../../gongxiaozhang.github.io
---



### 前言

2015年至今，人工智能（AI）芯片在学术界研究火热，典型的包括中科院的[DianNao](http://novel.ict.ac.cn/diannao/)（ASPLOS, 2014 最佳论文），以及Google的[TPU](https://arxiv.org/abs/1704.04760)（ISCA, 2017)。与其它芯片技术研究不同的是，这一领域在工业界迅速出现了大量的应用。谷歌先后发布的TPU、TPUv2、Cloud TPU等芯片产品，并宣称可以取得比Nvidia GPU更好的性能效率。特斯拉也发布了用于无人驾驶的AI芯片（2019.04），并号称是全球最强的AI芯片。与此同时，很多创业公司借着这波浪潮迅速成长，包括"寒武纪"（25亿美金，B轮，2018/06），以及"地平线"（30亿美元，B轮，2019/03）等等。

&emsp;  学术界和工业界如此火热的场景，吸引了一大批掘金者和弄潮儿，究其原因，个人认为主要是三点，"强需求"、"大前景"、"低门槛"。简单点说，就是一个产品，当前就有非常不错的应用市场需求，未来又有很大的ppt空间，实现难度也没那么高，所以吸引了这么多的玩家加入，也就不足为奇了。

<br/>

### 为什么大家都来做AI芯片？

1. ***强需求***&emsp;近年来，人工智能、深度学习等名词飞速发展，有话是“ AI is eating the software”，就是说用AI的算法取代之前所有的经验算法，虽然有些理想，但深度神经网络算法确实在各大互联网公司广泛部署，并在广告推荐、自然语言交互、图像处理、自动驾驶等等领域都取得了喜人的效果，而在这之前，还从没有哪一类算法，取得过如此广泛的应用和深远的影响。但是，蛋糕不是免费的，相比于传统算法，深度学习算法对计算力的要求，是指数级别的提升，传统的CPU远远不能满足深度学习算法对算力的需求。与此同时，针对大规模并行计算架构设计的GPU，意外地成为了AI计算的宠儿，主要玩家Nvidia的股价也是2年涨了10倍，市值最高达1800多亿美元（目前跌到900多亿）。每年BAT单家采购的GPU板卡多达数万块，一张Inference卡T4的价格在2W RBM左右，而一张training训练卡V100的价格在5W 左右，总体上说，当前AI计算芯片的每年的市场规模需求应该可以达到100亿美金，整个市场至少容得下总市值1000亿美金的企业，当然了，目前Nvidia仍然占据绝大多数。
2.  ***大前景***&emsp; 不用解释，最近两年还有比AI更火的topic吗？  Baidu说"all in AI"， 腾讯说"AI in all"，AI技术成为了人们憧憬未来的炼金术，而"AI芯片"，又是整个人工智能技术发展的基础。在各家的ppt中，AI芯片被广泛应用到了广告推荐、语音交互、图像、计算机视觉、AR/VR、手机、智能摄像头、IoT、自动驾驶等各大领域，这其中，每一个场景拿出来都可以是一个"big story"。与此同时，如果底层芯片能够取得突破性进展，必将反过来推动人工智能算法的跨越发展，从而似乎为"通用人工智能"指明了一条道路。
3. ***低门槛***&emsp; 不管是学术界，还是工业界，AI芯片的研究和实现，也就这两年的事情。当我们去看寒武纪或者Google的论文时，可以发现其架构和思路还是相对清晰的，不存在太多的黑科技，生产制造上也是通用的数字芯片工艺，找台积电生产就行。此外，当前市场还处于"婴儿期"，大家都在跑步入场，中美公司同时起步，没有谁是绝对的市场领导者，也没有谁有绝对的技术优势。正因为此，除了科技巨头（Google，BAT，Tesla），大量的AI芯片创业公司也如雨后春笋般涌现出来，"Computer Architecture"很久没有这么热闹了。总之呢，这是件好事，对世界以及我国半导体领域技术的进步，是有着十分的积极意义的。

<br/>

### 为什么AI芯片可以取得更好的性能？

体系结构宗师John Hennessy和David Patterson两人刚获得了2017年的图灵奖，他们合著的《Computer Architecture: A Quantitative Approach, Sixth Edition》一书中，新增加了一节"Domain-Specific Architectures", 对近两年火热的专用领域加速器做了详细的介绍，并分析了Google的TPU和微软的Catapult等AI加速器。根据大神们的观点，传统的CPU架构在AI时代已经过时，重新设计的AI处理器可以取得比传统CPU高几个数量级的性能。相比于传统CPU，针对AI的DSA（Domain-Specific Accelerator）有如下特点：

1. *使用专用的scratchpad memory来大大降低数据被move的次数和距离*

    传统的CPU采用多级的cache架构，但是cache的功耗是同等规模的scratchpad memory的两倍多。针对AI等具体应用的DSA，数据的move通常有明显的规律性，所以可以是software-controlled，就是由programmer或者compiler来完成，这样就不需要硬件来帮忙做数据的move。

2. *将有限的硬件资源投入到更多的算术单元和更大的memory*

    AI芯片，本质上还是各种门电路堆积起来，还是数字芯片，底层上并没有本质创新（当然也有一些模拟计算AI芯片、或者类人脑芯片，还处于学术研究阶段，本文不展开）。所以，要想取得100倍的计算提升，算术单元至少是传统CPU的100倍，而scratchpad memory至少是传统架构的10倍，这样才能解释得通。至于传统CPU/GPU的复杂的微架构体系，能删则删，比如复杂的乱序执行、多线程、多进程、prefetching、address coalescing等等，在AI计算领域可以节省或者简化。

3. *使用更小的数据类型，比如用int16/int8 来替代float32*

    这方面有大量的论文来论证，是可行的。

4. *充分挖掘计算的并行性，并尽可能使用一些简单的并行方法*

    AI应用大部分都是大规模矩阵计算，所以天然有很大的并行性，如何挖掘AI计算的并行性，是AI芯片设计的关键。通常的，大家使用SIMD来作data-level parallelism，用VLIW 来实现instruction-level parallelism。

5. *专用的编程语言和编译器，实现软硬件协同的优化*

    在保证用户效率的前提下，提供给用户简洁高效的使用接口。这才是考验AI芯片玩家实力的地方。

&emsp; 总之，AI芯片牺牲了一些编程方便性和灵活性，而将大量的资源用到了计算和memory上，在当前传统CPU发展停滞的背景下，是一种有效的尝试，也会激发出一些创新的硬件结构。但从上面的1～4点来看，设计一个AI芯片并不复杂，至少比通用的CPU/GPU简单多了（虽然我国已经努力了好多年），这也是为什么最近这么多创业公司跳出来做AI芯片的原因。

&emsp; 但是，个人认为，上面的第五点，就是整个软件栈，才是AI芯片最核心的竞争力，而且这里所需要的投入不容小觑，NVIDA的CUDA计划，至今已投入100亿美元。这两年，就是各家AI芯片交答卷的时候，"能不能用？好不好用？"是最关键的问题。

&emsp; 这波AI芯片浪潮，大家且行且珍惜。

<br/>

![image-20190621220637196](/assets/img/image-20190621220637196.png)

*refs:*

1. [人工智能芯片技术白皮书—清华大学](http://www.cbdio.com/image/site2/20181218/f42853157e261d82a71c33.pdf)
2. Computer Architecture: A Quantitative Approach, Sixth Edition

